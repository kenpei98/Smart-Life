{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading in PyTorch, using torchvision package. In here, we will be loading the CIFAR10 dataset.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10 #Load the dataset using torchvision\n",
    "from torchvision.transforms import transforms #Define transformations to be applied on the image\n",
    "from torch.utils.data import DataLoader#Create an instance of the DataLoader to hold the images\n",
    "\n",
    "#import Adam optimizer in order to training the model\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Under is the Unit class which is for modularity, we put convolution and relu in one single separate module and stack \n",
    "#much of this module in the SimpleNet class. We do that since we want make the code clean and not cumbersome.\n",
    "#This Unit class consist of convolution, batchNorm2d and relu. Batch Normalization essentially normalizes all inputs to\n",
    "#have zero mean and unit variance. It will greatly boosts the accuracy of the CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Unit, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, kernel_size=3, out_channels=out_channels, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.conv(intput)\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This SimpleNet class use above Unit class as sub-modules.\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet,self).__init__()\n",
    "        \n",
    "        self.unit1 = Unit(in_channels=3, out_channels=32)\n",
    "        self.unit2 = Unit(in_channels=32, out_channels=32)\n",
    "        self.unit3 = Unit(in_channels=32, out_channels=32)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit4 = Unit(in_channels=32, out_channels=64)\n",
    "        self.unit5 = Unit(in_channels=64, out_channels=64)\n",
    "        self.unit6 = Unit(in_channels=64, out_channels=64)\n",
    "        self.unit7 = Unit(in_channels=64, out_channels=64)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit8 = Unit(in_channels=64, out_channels=128)\n",
    "        self.unit9 = Unit(in_channels=128, out_channels=128)\n",
    "        self.unit10 = Unit(in_channels=128, out_channels=128)\n",
    "        self.unit11 = Unit(in_channels=128, out_channels=128)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit12 = Unit(in_channels=128, out_channels=128)\n",
    "        self.unit13 = Unit(in_channels=128, out_channels=128)\n",
    "        self.unit14 = Unit(in_channels=128, out_channels=128)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=4)\n",
    "        \n",
    "        #Add all the units into the Sequential layer in exact order\n",
    "        #Putting all layers except the fully connected layer(Linear layer) into a sequential class.\n",
    "        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, self.unit4, self.unit5, self.unit6, self.unit7,\n",
    "                                self.pool2, self.unit8, self.unit9, self.unit10, self.unit11, self.pool3, self.unit12, self.unit13,\n",
    "                                self.unit14, self.avgpool)\n",
    "        #The out_channels = 128, and after pooling 3 times, our 32*32 images have become 4*4, and after apply AvgPool2d\n",
    "        #of kernel size 4, it turning our feature map into 1*1*128, and the linear layer would have 1*1*128=128 input features\n",
    "        self.fc = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        \n",
    "        def forward(self, input):\n",
    "            output = self.net(input)\n",
    "            #flatten the output of the network to have 128 features.\n",
    "            output = output.view(-1,128)\n",
    "            output = self.fc(output)\n",
    "            return output\n",
    "        \n",
    "#This model contains 14 convolution layers, 14 ReLU layers, 14 batch normalization layers, 4 pooling layers and 1 linear layer.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomCrop(size=(32, 32), padding=4)\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define transformations for the training set. Firstly, we pass an array of transformations using transform.Compose, use RandomHorizontalFlip() to randomly flips the images horizontally, use\n",
    "#RandomCrop randomly crops the images. ToTensor() converts the images into a format usable by PyTorch. \n",
    "#Normalize() with the values given below will make all our pixels range between -1 to +1.\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    #In this transformation, ToTensor and Normalize must be at last in this order.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "train_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomCrop(size=(32, 32), padding=4)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the training set using CIFAR10 class\n",
    "train_set = CIFAR10(root=\"./data\", train=True, transform=train_transformations, download=True)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x12cd54950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#Create a loader for the training set with batch_size equal 32 images\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define transformations for the test set\n",
    "test_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "    \n",
    "])\n",
    "test_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the test set, the train in here should set to False\n",
    "test_set = CIFAR10(root=\"./data\", train=False, transform=test_transformations, download=True)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x128f39b50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a loader for the test set, shuffle in here set to false\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Under is the step for training the model:\n",
    "#Begining: from torch.optim import Adam \n",
    "#Instantiate the model and create the optimizer and loss function\n",
    "#Write a function to adjust learning rates\n",
    "#Write functions to save and evaluate the model\n",
    "#Write the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if gpu support is available\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "cuda_avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create model, optimizer and loss function\n",
    "model = SimpleNet(num_classes = 10)\n",
    "#if cuda is available, move the model to the GPU\n",
    "if cuda_avail:\n",
    "    model.cuda()\n",
    "#Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a learning rate adjustment function that divides the learning rate by 10 every 30 epochs\n",
    "def adjust_learning_rate(epoch):\n",
    "    \n",
    "    lr = 0.001\n",
    "    \n",
    "    if epoch > 180:\n",
    "        lr = lr / 1000000\n",
    "    elif epoch > 150:\n",
    "        lr = lr / 100000\n",
    "    elif epoch > 120:\n",
    "        lr = lr / 10000\n",
    "    elif epoch > 90:\n",
    "        lr = lr / 1000\n",
    "    elif epoch > 60:\n",
    "        lr = lr / 100\n",
    "    elif epoch > 30:\n",
    "        lr = lr / 10\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(epoch):\n",
    "    torch.save(model.state_dict(), \"cifar10model_{}.model\".format(epoch))\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To evaluate the accuracy of the model on the test set.\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    #We iterate over the test loader, each time, we will move the images and labels to the GPU, if GPU availble, we will \n",
    "    #wrap the images and labels in a Variable. The images then passed into the model to obtain predictions. The maximum predictions\n",
    "    #is picked and then compared to the actual class to obtain the accuracy. Finally, return the average accuracy.\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        \n",
    "        if cuda_avail:\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        #Predict classes using images from the test set\n",
    "        outputs = model(images)\n",
    "        _,prediction = torch.max(outputs.data, 1)\n",
    "        prediction = prediction.cpu().numpy()\n",
    "        ##problem\n",
    "        test_acc += torch.sum(torch.from_numpy(prediction).cuda() == labels.data)\n",
    "        \n",
    "        \n",
    "    #Compute the average acc and loss over all 10000 test images\n",
    "    test_acc = test_acc / 10000\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        #We loop over the loader for the training set\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            #Move images and labels to GPU if GPU support is available\n",
    "            if cuda_avail:\n",
    "                images = Variable(images.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "                \n",
    "            #Clear all accumulated gradients\n",
    "            #This step is important since weights in neural network are adjusted based on gradients accumulated for each batch\n",
    "            #For each new batch, gradients must be reset to zero, so images in a previous batch would not propagate gradients to\n",
    "            #a new batch.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Pass our images into the model we create, it returns the predictions, and then we pass both the predictions and actual labels\n",
    "            #into the loss function.\n",
    "            #Predict classes using images from the train set\n",
    "            outputs = model(images)\n",
    "            \n",
    "            #Compute the loss based on the predictions and actual labels\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            \n",
    "            #We call loss.backward() to propagate the gradients, and then we call optimizer.step() to modify our model parameters \n",
    "            #in accordance with the propagated gradients.\n",
    "            #Backpropagate the loss\n",
    "            loss.backward()\n",
    "            \n",
    "            #Adjust parameters according to the computed gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            #We retrieve the actual loss and then obtain the maximum predicted class.\n",
    "            #Problem\n",
    "            train_loss += loss.cpu().data* images.size(0)\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            \n",
    "            #We sum up the number of correct predictions in the batch and add it to the total train_acc.\n",
    "            train_acc += torch.sum(prediction == labels.data)\n",
    "        \n",
    "        \n",
    "        #call the learning rate adjustment function\n",
    "        adjust_learning_rate(epoch)\n",
    "        \n",
    "        #Compute the average training accuracy and training loss over all 50000 training images\n",
    "        train_acc = train_acc/50000\n",
    "        train_loss = train_loss/50000\n",
    "        \n",
    "        #Find the test accuracy \n",
    "        test_acc = test()\n",
    "        \n",
    "        #We keep track the best accuracy, and will call the save models if the current test accuracy is greater than our current best\n",
    "        if test_acc > best_acc:\n",
    "            save_models(epoch)\n",
    "            best_acc = test_acc\n",
    "            \n",
    "        #Print the metrics\n",
    "        print(\"Epoch {}, Train Accuracy:{}, TrainLoss: {}, Test Accuracy: {}\".format(epoch, train_acc, train_loss, test_acc))\n",
    "      \n",
    "\n",
    "    \n",
    "#problem\n",
    "#if __name__ == \"__main__\":\n",
    "    #train(200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
